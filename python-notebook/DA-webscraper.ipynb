{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Scraping</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class ImageItem(scrapy.Item):\n",
    "\n",
    "    # for downloading images in ImagePipeline\n",
    "    image_urls = scrapy.Field()\n",
    "    \n",
    "    #link to specific image\n",
    "    image_links = scrapy.Field()\n",
    "    \n",
    "    #image attributes\n",
    "    titles = scrapy.Field()\n",
    "    date_posted = scrapy.Field()\n",
    "    hashtags = scrapy.Field()\n",
    "    \n",
    "    #stats\n",
    "    views = scrapy.Field()\n",
    "    faves = scrapy.Field()\n",
    "    comments = scrapy.Field()\n",
    "    downloads = scrapy.Field()\n",
    "    \n",
    "    #artist details\n",
    "    artists = scrapy.Field()\n",
    "    artist_urls = scrapy.Field()\n",
    "    artist_deviations = scrapy.Field()\n",
    "    artist_comments = scrapy.Field()\n",
    "    artist_page_views = scrapy.Field()\n",
    "    artist_scraps = scrapy.Field()\n",
    "    artist_watchers = scrapy.Field()\n",
    "    artist_critiques = scrapy.Field()\n",
    "    artist_forum_posts = scrapy.Field()\n",
    "    artist_faves = scrapy.Field()\n",
    "    artist_asl = scrapy.Field() #age, sex, location\n",
    "    artist_dob = scrapy.Field() #date of birth\n",
    "    account_age = scrapy.Field() #how old the account is\n",
    "    membership = scrapy.Field() #artist current membership status\n",
    "    \n",
    "    # to be filled in by ImagePipeline\n",
    "    image_paths = scrapy.Field()\n",
    "    images = scrapy.Field()\n",
    "    \n",
    "class MyImagesPipeline(ImagesPipeline):\n",
    "\n",
    "    def get_media_requests(self, item, info):\n",
    "        for image_url in item['image_urls']:\n",
    "            yield scrapy.Request(image_url)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        image_paths = [x['path'] for ok, x in results if ok]\n",
    "        if not image_paths:\n",
    "            raise DropItem(\"Item contains no images\")\n",
    "        item['image_paths'] = image_paths\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 19:51:28 [scrapy.utils.log] INFO: Scrapy 1.5.2 started (bot: scrapybot)\n",
      "2019-04-17 19:51:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 17.5.0, Python 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.6.1, Platform Darwin-18.2.0-x86_64-i386-64bit\n",
      "2019-04-17 19:51:28 [scrapy.crawler] INFO: Overridden settings: {'DOWNLOAD_DELAY': 0.25, 'DOWNLOAD_FAIL_ON_DATALOSS': False, 'FEED_FORMAT': 'json', 'FEED_URI': 'image-data.json', 'LOG_LEVEL': 20}\n",
      "2019-04-17 19:51:28 [scrapy.extensions.telnet] INFO: Telnet Password: b6ed4b5034087165\n",
      "2019-04-17 19:51:28 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-04-17 19:51:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-04-17 19:51:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-04-17 19:51:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "['__main__.MyImagesPipeline']\n",
      "2019-04-17 19:51:28 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-04-17 19:51:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100 images...\n",
      "Downloaded 200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 19:52:28 [scrapy.extensions.logstats] INFO: Crawled 269 pages (at 269 pages/min), scraped 77 items (at 77 items/min)\n",
      "2019-04-17 19:53:28 [scrapy.extensions.logstats] INFO: Crawled 562 pages (at 293 pages/min), scraped 173 items (at 96 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 300 images...\n",
      "Downloaded 400 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 19:54:28 [scrapy.extensions.logstats] INFO: Crawled 844 pages (at 282 pages/min), scraped 259 items (at 86 items/min)\n",
      "2019-04-17 19:55:28 [scrapy.extensions.logstats] INFO: Crawled 1127 pages (at 283 pages/min), scraped 352 items (at 93 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 19:56:28 [scrapy.extensions.logstats] INFO: Crawled 1407 pages (at 280 pages/min), scraped 435 items (at 83 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 600 images...\n",
      "Downloaded 700 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 19:57:28 [scrapy.extensions.logstats] INFO: Crawled 1675 pages (at 268 pages/min), scraped 513 items (at 78 items/min)\n",
      "2019-04-17 19:58:28 [scrapy.extensions.logstats] INFO: Crawled 1956 pages (at 281 pages/min), scraped 600 items (at 87 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 19:59:28 [scrapy.extensions.logstats] INFO: Crawled 2233 pages (at 277 pages/min), scraped 680 items (at 80 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 900 images...\n",
      "Downloaded 1000 images...\n",
      "Downloaded 1100 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:00:28 [scrapy.extensions.logstats] INFO: Crawled 2512 pages (at 279 pages/min), scraped 764 items (at 84 items/min)\n",
      "2019-04-17 20:01:28 [scrapy.extensions.logstats] INFO: Crawled 2791 pages (at 279 pages/min), scraped 849 items (at 85 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1200 images...\n",
      "Downloaded 1300 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:02:28 [scrapy.extensions.logstats] INFO: Crawled 3063 pages (at 272 pages/min), scraped 926 items (at 77 items/min)\n",
      "2019-04-17 20:03:28 [scrapy.extensions.logstats] INFO: Crawled 3325 pages (at 262 pages/min), scraped 997 items (at 71 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1400 images...\n",
      "Downloaded 1500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:04:28 [scrapy.extensions.logstats] INFO: Crawled 3597 pages (at 272 pages/min), scraped 1070 items (at 73 items/min)\n",
      "2019-04-17 20:05:28 [scrapy.extensions.logstats] INFO: Crawled 3861 pages (at 264 pages/min), scraped 1143 items (at 73 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1600 images...\n",
      "Downloaded 1700 images...\n",
      "Downloaded 1800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:06:28 [scrapy.extensions.logstats] INFO: Crawled 4127 pages (at 266 pages/min), scraped 1215 items (at 72 items/min)\n",
      "2019-04-17 20:07:28 [scrapy.extensions.logstats] INFO: Crawled 4360 pages (at 233 pages/min), scraped 1278 items (at 63 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1900 images...\n",
      "Downloaded 2000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:08:28 [scrapy.extensions.logstats] INFO: Crawled 4624 pages (at 264 pages/min), scraped 1344 items (at 66 items/min)\n",
      "2019-04-17 20:09:28 [scrapy.extensions.logstats] INFO: Crawled 4897 pages (at 273 pages/min), scraped 1420 items (at 76 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2100 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:10:28 [scrapy.extensions.logstats] INFO: Crawled 5166 pages (at 269 pages/min), scraped 1493 items (at 73 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2200 images...\n",
      "Downloaded 2300 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:11:28 [scrapy.extensions.logstats] INFO: Crawled 5426 pages (at 260 pages/min), scraped 1555 items (at 62 items/min)\n",
      "2019-04-17 20:12:28 [scrapy.extensions.logstats] INFO: Crawled 5689 pages (at 263 pages/min), scraped 1628 items (at 73 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2400 images...\n",
      "Downloaded 2500 images...\n",
      "Downloaded 2600 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:13:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.deviantart.com/sherlock-holmes> (referer: https://www.deviantart.com/sherlock-holmes/art/Sherlock-Holmes-Club-ID-8867259)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 653, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-3-1d96902a9d4a>\", line 118, in parse_artist\n",
      "    age = response.xpath(\"//a[@href='#super-secret-activity']/span/div/text()\").extract()[0]\n",
      "IndexError: list index out of range\n",
      "2019-04-17 20:13:28 [scrapy.extensions.logstats] INFO: Crawled 5942 pages (at 253 pages/min), scraped 1685 items (at 57 items/min)\n",
      "2019-04-17 20:14:28 [scrapy.extensions.logstats] INFO: Crawled 6209 pages (at 267 pages/min), scraped 1756 items (at 71 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2700 images...\n",
      "Downloaded 2800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:15:28 [scrapy.extensions.logstats] INFO: Crawled 6470 pages (at 261 pages/min), scraped 1823 items (at 67 items/min)\n",
      "2019-04-17 20:16:28 [scrapy.extensions.logstats] INFO: Crawled 6729 pages (at 259 pages/min), scraped 1887 items (at 64 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2900 images...\n",
      "Downloaded 3000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:17:28 [scrapy.extensions.logstats] INFO: Crawled 6993 pages (at 264 pages/min), scraped 1952 items (at 65 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3100 images...\n",
      "Downloaded 3200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:18:28 [scrapy.extensions.logstats] INFO: Crawled 7255 pages (at 262 pages/min), scraped 2019 items (at 67 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3300 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:19:28 [scrapy.extensions.logstats] INFO: Crawled 7505 pages (at 250 pages/min), scraped 2079 items (at 60 items/min)\n",
      "2019-04-17 20:20:28 [scrapy.extensions.logstats] INFO: Crawled 7765 pages (at 260 pages/min), scraped 2144 items (at 65 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3400 images...\n",
      "Downloaded 3500 images...\n",
      "Downloaded 3600 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:21:28 [scrapy.extensions.logstats] INFO: Crawled 8015 pages (at 250 pages/min), scraped 2200 items (at 56 items/min)\n",
      "2019-04-17 20:22:28 [scrapy.extensions.logstats] INFO: Crawled 8266 pages (at 251 pages/min), scraped 2258 items (at 58 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3700 images...\n",
      "Downloaded 3800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:23:28 [scrapy.extensions.logstats] INFO: Crawled 8522 pages (at 256 pages/min), scraped 2315 items (at 57 items/min)\n",
      "2019-04-17 20:24:28 [scrapy.extensions.logstats] INFO: Crawled 8777 pages (at 255 pages/min), scraped 2378 items (at 63 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 3900 images...\n",
      "Downloaded 4000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:25:28 [scrapy.extensions.logstats] INFO: Crawled 9032 pages (at 255 pages/min), scraped 2438 items (at 60 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 4100 images...\n",
      "Downloaded 4200 images...\n",
      "Downloaded 4300 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:26:28 [scrapy.extensions.logstats] INFO: Crawled 9279 pages (at 247 pages/min), scraped 2489 items (at 51 items/min)\n",
      "2019-04-17 20:27:28 [scrapy.extensions.logstats] INFO: Crawled 9528 pages (at 249 pages/min), scraped 2544 items (at 55 items/min)\n",
      "2019-04-17 20:28:28 [scrapy.extensions.logstats] INFO: Crawled 9777 pages (at 249 pages/min), scraped 2601 items (at 57 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 4400 images...\n",
      "Downloaded 4500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:29:28 [scrapy.extensions.logstats] INFO: Crawled 10031 pages (at 254 pages/min), scraped 2658 items (at 57 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 4600 images...\n",
      "Downloaded 4700 images...\n",
      "Downloaded 4800 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:30:28 [scrapy.extensions.logstats] INFO: Crawled 10282 pages (at 251 pages/min), scraped 2709 items (at 51 items/min)\n",
      "2019-04-17 20:31:28 [scrapy.extensions.logstats] INFO: Crawled 10526 pages (at 244 pages/min), scraped 2758 items (at 49 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 4900 images...\n",
      "Downloaded 5000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:32:28 [scrapy.extensions.logstats] INFO: Crawled 10773 pages (at 247 pages/min), scraped 2809 items (at 51 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5100 images...\n",
      "Downloaded 5200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:33:28 [scrapy.extensions.logstats] INFO: Crawled 11018 pages (at 245 pages/min), scraped 2860 items (at 51 items/min)\n",
      "2019-04-17 20:34:28 [scrapy.extensions.logstats] INFO: Crawled 11267 pages (at 249 pages/min), scraped 2913 items (at 53 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5300 images...\n",
      "Downloaded 5400 images...\n",
      "Downloaded 5500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:35:28 [scrapy.extensions.logstats] INFO: Crawled 11518 pages (at 251 pages/min), scraped 2963 items (at 50 items/min)\n",
      "2019-04-17 20:36:28 [scrapy.extensions.logstats] INFO: Crawled 11761 pages (at 243 pages/min), scraped 3013 items (at 50 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5600 images...\n",
      "Downloaded 5700 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:37:28 [scrapy.extensions.logstats] INFO: Crawled 12007 pages (at 246 pages/min), scraped 3060 items (at 47 items/min)\n",
      "2019-04-17 20:38:28 [scrapy.extensions.logstats] INFO: Crawled 12243 pages (at 236 pages/min), scraped 3102 items (at 42 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 5800 images...\n",
      "Downloaded 5900 images...\n",
      "Downloaded 6000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:39:28 [scrapy.extensions.logstats] INFO: Crawled 12490 pages (at 247 pages/min), scraped 3154 items (at 52 items/min)\n",
      "2019-04-17 20:39:39 [scrapy.pipelines.files] ERROR: File (unknown-error): Error processing file from <GET https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0dc70551-b190-44e8-9b59-5e9378a30049/d2y7tid-cc06bf13-115d-41d9-b9ec-00c33c8cab98.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzBkYzcwNTUxLWIxOTAtNDRlOC05YjU5LTVlOTM3OGEzMDA0OVwvZDJ5N3RpZC1jYzA2YmYxMy0xMTVkLTQxZDktYjllYy0wMGMzM2M4Y2FiOTguanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.3RRjMd6_q6cBQZh-YJCavU7n72FfQbHmg1KuoeJpdk8> referred in <None>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1386, in _inlineCallbacks\n",
      "    result = g.send(result)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/scrapy/core/downloader/middleware.py\", line 43, in process_request\n",
      "    defer.returnValue((yield download_func(request=request,spider=spider)))\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/twisted/internet/defer.py\", line 1363, in returnValue\n",
      "    raise _DefGen_Return(val)\n",
      "twisted.internet.defer._DefGen_Return: <200 https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0dc70551-b190-44e8-9b59-5e9378a30049/d2y7tid-cc06bf13-115d-41d9-b9ec-00c33c8cab98.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzBkYzcwNTUxLWIxOTAtNDRlOC05YjU5LTVlOTM3OGEzMDA0OVwvZDJ5N3RpZC1jYzA2YmYxMy0xMTVkLTQxZDktYjllYy0wMGMzM2M4Y2FiOTguanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.3RRjMd6_q6cBQZh-YJCavU7n72FfQbHmg1KuoeJpdk8>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/scrapy/pipelines/files.py\", line 401, in media_downloaded\n",
      "    checksum = self.file_downloaded(response, request, info)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/scrapy/pipelines/images.py\", line 101, in file_downloaded\n",
      "    return self.image_downloaded(response, request, info)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/scrapy/pipelines/images.py\", line 105, in image_downloaded\n",
      "    for path, image, buf in self.get_images(response, request, info):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/scrapy/pipelines/images.py\", line 125, in get_images\n",
      "    image, buf = self.convert_image(orig_image)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/scrapy/pipelines/images.py\", line 151, in convert_image\n",
      "    image.save(buf, 'JPEG')\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1899, in save\n",
      "    self.load()\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 228, in load\n",
      "    \"(%d bytes not processed)\" % len(b))\n",
      "OSError: image file is truncated (50 bytes not processed)\n",
      "2019-04-17 20:39:40 [scrapy.core.scraper] WARNING: Dropped: Item contains no images\n",
      "{'account_age': 'Deviant for 15  Years',\n",
      " 'artist_asl': 'Philippines',\n",
      " 'artist_comments': '731 ',\n",
      " 'artist_critiques': '0 ',\n",
      " 'artist_deviations': '75 ',\n",
      " 'artist_faves': '100 ',\n",
      " 'artist_forum_posts': '1 ',\n",
      " 'artist_page_views': '38,868 ',\n",
      " 'artist_scraps': '0 ',\n",
      " 'artist_urls': 'https://www.deviantart.com/melrosestormhaven',\n",
      " 'artist_watchers': '454 ',\n",
      " 'artists': 'melrosestormhaven',\n",
      " 'comments': '678',\n",
      " 'date_posted': 'September 7, 2010',\n",
      " 'downloads': '818',\n",
      " 'faves': '4857',\n",
      " 'image_links': 'https://www.deviantart.com/melrosestormhaven/art/BBC-Sherlock-and-John-178404133',\n",
      " 'image_urls': ['https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/0dc70551-b190-44e8-9b59-5e9378a30049/d2y7tid-cc06bf13-115d-41d9-b9ec-00c33c8cab98.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzBkYzcwNTUxLWIxOTAtNDRlOC05YjU5LTVlOTM3OGEzMDA0OVwvZDJ5N3RpZC1jYzA2YmYxMy0xMTVkLTQxZDktYjllYy0wMGMzM2M4Y2FiOTguanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.3RRjMd6_q6cBQZh-YJCavU7n72FfQbHmg1KuoeJpdk8'],\n",
      " 'titles': 'BBC Sherlock and John',\n",
      " 'views': '58447'}\n",
      "2019-04-17 20:40:28 [scrapy.extensions.logstats] INFO: Crawled 12736 pages (at 246 pages/min), scraped 3203 items (at 49 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 6100 images...\n",
      "Downloaded 6200 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:41:28 [scrapy.extensions.logstats] INFO: Crawled 12989 pages (at 253 pages/min), scraped 3262 items (at 59 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 6300 images...\n",
      "Downloaded 6400 images...\n",
      "Downloaded 6500 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:42:28 [scrapy.extensions.logstats] INFO: Crawled 13225 pages (at 236 pages/min), scraped 3304 items (at 42 items/min)\n",
      "2019-04-17 20:43:28 [scrapy.extensions.logstats] INFO: Crawled 13471 pages (at 246 pages/min), scraped 3357 items (at 53 items/min)\n",
      "2019-04-17 20:44:27 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-04-17 20:44:27 [scrapy.extensions.feedexport] INFO: Stored json feed (3406 items) in: image-data.json\n",
      "2019-04-17 20:44:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 9323610,\n",
      " 'downloader/request_count': 13706,\n",
      " 'downloader/request_method_count/GET': 13706,\n",
      " 'downloader/response_bytes': 1136183374,\n",
      " 'downloader/response_count': 13706,\n",
      " 'downloader/response_status_count/200': 13706,\n",
      " 'dupefilter/filtered': 3158,\n",
      " 'file_count': 3407,\n",
      " 'file_status_count/downloaded': 3407,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 4, 17, 23, 44, 27, 187626),\n",
      " 'item_dropped_count': 1,\n",
      " 'item_dropped_reasons_count/DropItem': 1,\n",
      " 'item_scraped_count': 3406,\n",
      " 'log_count/ERROR': 2,\n",
      " 'log_count/INFO': 61,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 239230976,\n",
      " 'memusage/startup': 104951808,\n",
      " 'request_depth_max': 3,\n",
      " 'response_received_count': 13706,\n",
      " 'scheduler/dequeued': 10299,\n",
      " 'scheduler/dequeued/memory': 10299,\n",
      " 'scheduler/enqueued': 10299,\n",
      " 'scheduler/enqueued/memory': 10299,\n",
      " 'spider_exceptions/IndexError': 1,\n",
      " 'start_time': datetime.datetime(2019, 4, 17, 22, 51, 28, 193794)}\n",
      "2019-04-17 20:44:27 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "class ImageSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'images'\n",
    "    \n",
    "    start_urls = ['https://www.deviantart.com/popular-all-time/?q=sherlock&offset=0']\n",
    "    \n",
    "    \n",
    "    #initialize offset at 0\n",
    "    offset = 0\n",
    "    #set offset limit to control the amount of images downloaded\n",
    "    offset_limit = 8000\n",
    "    \n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.INFO,\n",
    "        'ITEM_PIPELINES': {'__main__.MyImagesPipeline': 1}, #enable image download\n",
    "        'IMAGES_STORE': 'DA-images', #store images in DA-images folder\n",
    "        'FEED_FORMAT':'json',                                \n",
    "        'FEED_URI': 'image-data.json', #store image data in image-data.json\n",
    "        'DOWNLOAD_FAIL_ON_DATALOSS': False, #if image download fails (due to various issues), don't send error message, just flag it.\n",
    "        'DOWNLOAD_DELAY': 0.25 #250 ms download delay, with inbuilt scrapy randomization\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        #get page body\n",
    "        page = response.css('div.page-results span.thumb')\n",
    "        \n",
    "        for img in page:\n",
    "            \n",
    "            #thumbnail link. If there isn't a thumbnail, then post is not an image and should be skipped\n",
    "            thumbnail = img.css('::attr(data-super-img)').get()\n",
    "            \n",
    "            #img_link contains the full link that leads to the individual image post\n",
    "            img_link = img.css('::attr(href)').get()\n",
    "            \n",
    "            #if there is a thumbnail, aka the post is an image, follow url to parse image for details\n",
    "            if thumbnail: \n",
    "                yield scrapy.Request(img_link, callback = self.parse_image)\n",
    "                \n",
    "        #go to next page\n",
    "        while self.offset < self.offset_limit:\n",
    "            self.offset += 24 #DA's natural offset scroll is set at increments of 24\n",
    "            next_page = f'https://www.deviantart.com/popular-all-time/?q=sherlock&offset={self.offset}'\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "            \n",
    "    def parse_image(self, response):\n",
    "        \n",
    "        #initialize image item\n",
    "        image = ImageItem()\n",
    "        \n",
    "        #get image url (for downloading via ImagePipeline)\n",
    "        image[\"image_urls\"] = [response.css('div.dev-view-deviation img ::attr(src)').get()]\n",
    "        #get other image info\n",
    "        image[\"image_links\"] = response.url\n",
    "        image['titles'] = response.xpath(\"//a[@class='title']/text()\").extract()[0]\n",
    "        image['date_posted'] = response.xpath(\"//div[@class='dev-right-bar-content dev-metainfo-content dev-metainfo-details']/dl/dd/span/text()\").extract()[0]\n",
    "        \n",
    "        #check whether image has hashtags (some don't)\n",
    "        hashtag = response.xpath(\"//div[@class='dev-about-tags-cc dev-about-breadcrumb']/a/text()\").extract()\n",
    "        if hashtag: image['hashtags'] = hashtag\n",
    "        \n",
    "        #get image stats\n",
    "        stats =  response.xpath(\"//div[@class='dev-right-bar-content dev-metainfo-content dev-metainfo-stats']/dl/dd/text()\").extract()\n",
    "        \n",
    "        #check that stats list only contains numbers (sometimes irregular data falls in)\n",
    "        stats = [re.sub(\"\\D\", \"\", s) for s in stats]\n",
    "        \n",
    "        #remove any None types from list\n",
    "        stats = list(filter(None, stats))\n",
    "        \n",
    "        #the responses are ordered in: views, faves, comments, downloads\n",
    "        #sometimes comments are disabled, sometimes downloads are disabled\n",
    "        headers = ['views','faves','comments','downloads']\n",
    "        \n",
    "        #if comments/downloads are disabled, they will not be looped over for a given image\n",
    "        for i in range(len(stats)):\n",
    "            image[headers[i]] = stats[i]\n",
    "            \n",
    "        #get artist info\n",
    "        artist_name = response.xpath(\"//small[@class='author']/span/a/text()\").extract()\n",
    "        \n",
    "        if artist_name: \n",
    "            image['artists'] = response.xpath(\"//small[@class='author']/span/a/text()\").extract()[-1]\n",
    "            image['artist_urls'] = response.xpath(\"//small[@class='author']/span/a/@href\").extract()[-1]     \n",
    "            request = scrapy.Request(image['artist_urls'], callback=self.parse_artist, meta={'image':image})\n",
    "            yield request\n",
    "        else: #if no artist name (sometimes artists are banned), just yield the image\n",
    "            image['artists'] = 'Banned'\n",
    "            yield image\n",
    "        \n",
    "    def parse_artist(self, response):\n",
    "        \n",
    "        #get image item for the higher-level parser\n",
    "        image = response.meta['image']\n",
    "        \n",
    "        #get artist account stats\n",
    "        artist_stats = response.xpath(\"//div[@id='super-secret-stats']/div/div/div/strong/text()\").extract()\n",
    "        \n",
    "        headers = ['artist_deviations','artist_comments','artist_page_views','artist_scraps','artist_watchers','artist_critiques','artist_forum_posts','artist_faves']\n",
    "        \n",
    "        for i in range(len(artist_stats)):\n",
    "            image[headers[i]] = artist_stats[i]\n",
    "        \n",
    "        #get account age and membership details\n",
    "        age_membership = response.xpath(\"//a[@href='#super-secret-activity']/div/text()\").extract()\n",
    "        \n",
    "        #sometimes the age is wrapped up in a span\n",
    "        if len(age_membership) == 0: \n",
    "            age = response.xpath(\"//a[@href='#super-secret-activity']/span/div/text()\").extract()[0]\n",
    "            age_membership.append(age)        \n",
    "        image['account_age'] = age_membership[0]\n",
    "\n",
    "        \n",
    "        #get artist personal details\n",
    "        artist_details = response.xpath(\"//div[@id='super-secret-why']/div/div/div/dl/dd/text()\").extract()\n",
    "        details = ['artist_asl','artist_dob'] #some artists do not share their dobs\n",
    "        for i in range(len(artist_details)):\n",
    "            image[details[i]] = artist_details[i]\n",
    "        \n",
    "        return image\n",
    "            \n",
    "process = CrawlerProcess()\n",
    "process.crawl(ImageSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_json('image-data.json')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
