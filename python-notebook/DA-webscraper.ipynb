{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Scraping</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class ImageItem(scrapy.Item):\n",
    "\n",
    "    #direct link to image file for downloading via ImagePipeline\n",
    "    image_urls = scrapy.Field()\n",
    "    \n",
    "    #link to specific image page for scraping more stats\n",
    "    page_links = scrapy.Field()\n",
    "    \n",
    "    #image attributes\n",
    "    titles = scrapy.Field() #image title\n",
    "    date_posted = scrapy.Field() #date posted\n",
    "    hashtags = scrapy.Field() #hashtags\n",
    "    \n",
    "    #image stats\n",
    "    faves = scrapy.Field() #number of faves of the current image\n",
    "    comments = scrapy.Field() #number of comments of the current image\n",
    "    views = scrapy.Field() #number of views of the current image\n",
    "    \n",
    "    #artist details\n",
    "    artists = scrapy.Field() #artist's name\n",
    "    artist_urls = scrapy.Field() #link to the artist's account\n",
    "    artist_page_views = scrapy.Field() #number of total page views\n",
    "    artist_deviations = scrapy.Field() #number of images posted by the artist\n",
    "    artist_watchers = scrapy.Field() #number of accounts following the artist\n",
    "    artist_watching = scrapy.Field() #number of accounts the artist is following\n",
    "    artist_favourites = scrapy.Field() #number of total faves received\n",
    "    artist_comments_made = scrapy.Field() #number of comments made\n",
    "    artist_comments_received = scrapy.Field() #number of total comments received\n",
    "\n",
    "    artist_account_age = scrapy.Field() #account age\n",
    "    \n",
    "    # to be filled in by ImagePipeline\n",
    "    image_paths = scrapy.Field() #location of image in local storage\n",
    "    \n",
    "    \n",
    "class MyImagesPipeline(ImagesPipeline):\n",
    "    '''\n",
    "    Image pipeline for downloading images.\n",
    "    '''\n",
    "    def get_media_requests(self, item, info):\n",
    "        for image_url in item['image_urls']:\n",
    "            yield scrapy.Request(image_url)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        image_paths = [x['path'] for ok, x in results if ok]\n",
    "        if not image_paths:\n",
    "            raise DropItem(\"Item contains no images\")\n",
    "        item['image_paths'] = image_paths\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImageSpider(scrapy.Spider):\n",
    "    \n",
    "    name = 'images'\n",
    "    \n",
    "    start_urls = ['https://www.deviantart.com/search/deviations/visual-art/original-work?order=popular-all-time&page=0&q=cyberpunk']\n",
    "    \n",
    "    #initialize page at 0\n",
    "    page = 0\n",
    "    #set page limit to control the amount of images downloaded\n",
    "    page_limit = 3\n",
    "    \n",
    "\n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.INFO,\n",
    "        'ITEM_PIPELINES': {'__main__.MyImagesPipeline': 1}, #enable image download\n",
    "        'IMAGES_STORE': 'cyberpunk/images', #store images \n",
    "        'FEED_FORMAT':'json',                                \n",
    "        'FEED_URI': 'cyberpunk/image-data.json', #store image data\n",
    "        'DOWNLOAD_FAIL_ON_DATALOSS': False, #if image download fails (due to various issues), don't send error message, just flag it.\n",
    "        #'DOWNLOAD_DELAY': 0.25 #250 ms download delay, with inbuilt scrapy randomization\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        #get list of image links from the page\n",
    "        img_links = response.css('div[class=_2tv7Y] a[data-hook=\"deviation_link\"]::attr(href)').getall()\n",
    "        \n",
    "        for link in img_links:\n",
    "            \n",
    "            yield scrapy.Request(link, callback = self.parse_image)\n",
    "                \n",
    "        #go to next page\n",
    "        while self.page < self.page_limit:\n",
    "            self.page += 1 #increment page by 1\n",
    "            next_page = f'https://www.deviantart.com/search/deviations/visual-art/original-work?order=popular-all-time&page={self.page}&q=cyberpunk'\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "        \n",
    "    def parse_image(self, response):\n",
    "        \n",
    "        #initialize image item\n",
    "        image = ImageItem()\n",
    "        \n",
    "        #get image url (for downloading via ImagePipeline)\n",
    "        image[\"image_urls\"] = [response.css('div[data-hook=\"art_stage\"] img::attr(src)').get()]\n",
    "        #get other image info\n",
    "        image[\"page_links\"] = response.url\n",
    "        image['titles'] = response.css('div[class=\"_3qGVQ\"]::text').get()\n",
    "        image['date_posted'] = response.css('div[class=\"_3XxFW\"]::text').getall()[-1]\n",
    "        \n",
    "        #check whether image has hashtags (some don't)\n",
    "        hashtag = response.css('div[class=\"_2ogLQ\"] span::text').getall()\n",
    "        if hashtag: image['hashtags'] = hashtag\n",
    "        \n",
    "        #get image stats\n",
    "        stats =  response.css('div[class=\"hYJJ_\"] span::text').getall()\n",
    "        \n",
    "        #remove blanks from list and limit to first 3 items (4th item onwards is irrelevant)\n",
    "        stats = ''.join(stats).split()[:3]\n",
    "        \n",
    "        #the responses are ordered in: faves, comments, views\n",
    "        image['faves'] = stats[0]\n",
    "        image['comments'] = stats[1]\n",
    "        image['views'] = stats[2]\n",
    "            \n",
    "        #get artist info\n",
    "        artist_name = response.css('a[data-hook=\"user_link\"]::attr(title)').get()\n",
    "        \n",
    "        if artist_name: \n",
    "            image['artists'] = response.css('a[data-hook=\"user_link\"]::attr(title)').get()\n",
    "            artist_gallery = response.css('a[data-hook=\"user_link\"]::attr(href)').get() \n",
    "            image['artist_urls'] = artist_gallery.replace('gallery','about') #replace the /gallery pointer to /about   \n",
    "            request = scrapy.Request(image['artist_urls'], callback=self.parse_artist, meta={'image':image})\n",
    "            yield request\n",
    "        else: #if no artist name (sometimes artists are banned), just yield the image\n",
    "            image['artists'] = 'Banned'\n",
    "            yield image\n",
    "            \n",
    "    def parse_artist(self, response):\n",
    "        \n",
    "        #get image item for the higher-level parser\n",
    "        image = response.meta['image']\n",
    "        \n",
    "        #get stat data\n",
    "        artist_stats = response.css('div[class=\"_1loOw\"]::text').getall()\n",
    "        \n",
    "        #get stat headers\n",
    "        headers = response.css('div[class=\"_1loOw\"] span::text').getall()\n",
    "        #prefix 'artist', lowercase, and replace blank space with underscore to make headers neat\n",
    "        headers = ['artist_' + s.lower().replace(' ','_') for s in headers]\n",
    "        \n",
    "        #assign stats to headers\n",
    "        for i in range(len(artist_stats)):\n",
    "            image[headers[i]] = artist_stats[i]\n",
    "        \n",
    "        #get artist personal info\n",
    "        personal_info = response.css('div[class=\"_2B4Yo _3N4ed\"] span::text').getall()     \n",
    "        image['artist_account_age'] = personal_info[-1]\n",
    "        \n",
    "        return image\n",
    "            \n",
    "process = CrawlerProcess()\n",
    "process.crawl(ImageSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('cyberpunk/image-data.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
